{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bank Customer Churn Prediction System**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:45:58.253807Z",
     "start_time": "2025-02-19T19:45:57.498170Z"
    }
   },
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from scipy.stats import randint, uniform\n",
    "from pandas.api.types import is_numeric_dtype, is_object_dtype\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ML Models\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Plotting Libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:45:58.272750Z",
     "start_time": "2025-02-19T19:45:58.268145Z"
    }
   },
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:45:58.322943Z",
     "start_time": "2025-02-19T19:45:58.283138Z"
    }
   },
   "source": [
    "dataset = pd.read_csv('./datasets/churn_dataset.csv')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:45:58.613354Z",
     "start_time": "2025-02-19T19:45:58.606838Z"
    }
   },
   "source": [
    "#FIXME: Fix the dataset\n",
    "#NOTE: This is a two databases combination problem not severe!\n",
    "dataset['Card Type '] = dataset['Card Type '].where(dataset['HasCrCard'] == 1, 'None')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:45:58.647960Z",
     "start_time": "2025-02-19T19:45:58.640448Z"
    }
   },
   "source": [
    "dataset.columns.tolist()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CustomerId',\n",
       " 'Surname',\n",
       " 'Education',\n",
       " 'CreditScore',\n",
       " 'Geography',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Tenure',\n",
       " 'Balance',\n",
       " 'NumOfProducts',\n",
       " 'HasCrCard',\n",
       " 'Card Type ',\n",
       " 'IsActiveMember',\n",
       " 'EstimatedSalary',\n",
       " 'Housing',\n",
       " 'Loan',\n",
       " 'Exited']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:45:58.754445Z",
     "start_time": "2025-02-19T19:45:58.744889Z"
    }
   },
   "source": [
    "dataset = dataset.rename(\n",
    "    columns={\n",
    "        'Education' : 'education',\n",
    "        'CreditScore' : 'credit_score',\n",
    "        'Geography' : 'geography',\n",
    "        'Gender' : 'gender',\n",
    "        'Age' : 'age',\n",
    "        'Tenure' : 'tenure',\n",
    "        'Balance' : 'balance',\n",
    "        'NumOfProducts' : 'num_of_products',\n",
    "        'HasCrCard' : 'has_cr_card',\n",
    "        'Card Type ' : 'card_type',\n",
    "        'IsActiveMember' : 'is_active_member',\n",
    "        'EstimatedSalary' : 'estimated_salary',\n",
    "        'Housing' : 'housing',\n",
    "        'Loan' : 'loan',\n",
    "        'Exited' : 'exited',\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:45:58.789843Z",
     "start_time": "2025-02-19T19:45:58.778178Z"
    }
   },
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    def handle_outliers(dataset):\n",
    "        # Note: Dropped columns are categorical features. There is no use to handle outliers for them.\n",
    "        for index, feature in enumerate(dataset.drop(columns=['education', 'geography', 'gender', 'card_type', 'is_active_member', 'has_cr_card', 'housing', 'loan', 'exited'])):\n",
    "            Q1 = dataset[feature].quantile(0.25)\n",
    "            Q3 = dataset[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            dataset[feature] = np.clip(dataset[feature], Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)\n",
    "          \n",
    "        return dataset\n",
    "\n",
    "    def handle_missing_values(dataset):\n",
    "        columns = dataset.columns.tolist()\n",
    "        for column in columns:\n",
    "            # Calculate null values and percentage\n",
    "            null_count = dataset[column].isnull().sum()\n",
    "            total_count = len(dataset[column])\n",
    "            null_percentage = (null_count / total_count) * 100\n",
    "\n",
    "            # Handle columns based on null percentage and data type\n",
    "            if null_percentage < 50:\n",
    "                # Fill missing values for numerical columns\n",
    "                if is_numeric_dtype(dataset[column]):\n",
    "                    mean = dataset[column].mean()\n",
    "                    dataset[column].fillna(mean, inplace=True)\n",
    "                # Fill missing values for categorical columns\n",
    "                elif is_object_dtype(dataset[column]):\n",
    "                    mode = dataset[column].mode()[0]\n",
    "                    dataset[column].fillna(mode, inplace=True)\n",
    "            # elif 50 <= null_percentage < 70:\n",
    "            #     # TODO: Implement the data missing data handling\n",
    "            #     print(f\"Under development for column: {column}\")\n",
    "            else:   \n",
    "                # Drop columns with more than 80% missing values\n",
    "                dataset.drop(columns=column, inplace=True)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def remove_duplicates(dataset):\n",
    "        if dataset.duplicated().sum() > 0:\n",
    "            dataset = dataset.drop_duplicates().reset_index(drop = True)\n",
    "        return dataset\n",
    "\n",
    "    def encode_categorical_features(dataset):\n",
    "        # One-Hot encoding\n",
    "        oneHotEncoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "        # Transform and convert to DataFrame\n",
    "        encoded = oneHotEncoder.fit_transform(dataset[['geography', 'education', 'card_type']])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=oneHotEncoder.get_feature_names_out(['geography', 'education', 'card_type']))\n",
    "\n",
    "        # Reset index before concatenation\n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "        encoded_df = encoded_df.reset_index(drop=True)\n",
    "\n",
    "        # Concatenate DataFrames\n",
    "        dataset = pd.concat([dataset, encoded_df], axis=1)\n",
    "\n",
    "        # Drop original categorical columns\n",
    "        dataset = dataset.drop(columns=['geography', 'education', 'card_type'])\n",
    "\n",
    "        # # Label encoding\n",
    "        gender_encoder = LabelEncoder()\n",
    "        housing_encoder = LabelEncoder()\n",
    "        loan_encoder = LabelEncoder()\n",
    "\n",
    "        # Fitting and transforming each column separately\n",
    "        dataset['gender'] = gender_encoder.fit_transform(dataset['gender'])\n",
    "        dataset['housing'] = housing_encoder.fit_transform(dataset['housing'])\n",
    "        dataset['loan'] = loan_encoder.fit_transform(dataset['loan'])\n",
    "\n",
    "        dataset['gender'] = dataset['gender'].astype(float)\n",
    "        dataset['housing'] = dataset['housing'].astype(float)\n",
    "        dataset['loan'] = dataset['loan'].astype(float)\n",
    "\n",
    "        # Export all the encoders\n",
    "        joblib.dump(oneHotEncoder, './models/onehot_encoder.pkl')\n",
    "        joblib.dump(gender_encoder, './models/gender_encoder.pkl')\n",
    "        joblib.dump(housing_encoder, './models/housing_encoder.pkl')\n",
    "        joblib.dump(loan_encoder, './models/loan_encoder.pkl')\n",
    "\n",
    "        # Moving the Y predictor to the end of the dataset\n",
    "        feature_exited = dataset['exited']\n",
    "        dataset = dataset.drop(columns=['exited'])\n",
    "        dataset = pd.concat([dataset, feature_exited], axis=1)\n",
    "\n",
    "        dataset.columns = dataset.columns.str.strip()\n",
    "        return dataset\n",
    "\n",
    "    def split_dataset_to_X_y(dataset):\n",
    "        X = dataset.iloc[:, :-1]\n",
    "        y = dataset.iloc[:, -1]\n",
    "        return X, y\n",
    "\n",
    "    def handle_class_imbalance(X, y):\n",
    "        categorical_features = [\n",
    "            'gender',\n",
    "            'has_cr_card', 'is_active_member',\n",
    "            'housing', 'loan','geography_Germany',\n",
    "            'geography_Spain', 'education_secondary',\n",
    "            'education_tertiary', 'education_unknown',\n",
    "            'card_type_GOLD','card_type_None',\n",
    "            'card_type_PLATINUM', 'card_type_SILVER'\n",
    "        ]\n",
    "\n",
    "        cat_indices = [X.columns.get_loc(col) for col in categorical_features]\n",
    "        \n",
    "        smote = SMOTENC(categorical_features=cat_indices, k_neighbors=9, random_state=42)\n",
    "        overSampled_X, overSampled_y = smote.fit_resample(X, y)\n",
    "\n",
    "        return overSampled_X, overSampled_y  \n",
    "\n",
    "    def scale_features(X_train, X_test):\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Export scaler\n",
    "        joblib.dump(scaler, './models/minMax_scaler.pkl')\n",
    "        return X_train_scaled, X_test_scaled\n",
    "\n",
    "    # Removing irrelevant features\n",
    "    dataset = dataset.drop(columns= ['CustomerId', 'Surname'])\n",
    "\n",
    "    # Removing the white spaces from feature names\n",
    "    dataset.columns = dataset.columns.str.strip()\n",
    "\n",
    "    # Handling null values\n",
    "    dataset = handle_missing_values(dataset)\n",
    "\n",
    "    # Check and drop duplicates from the database\n",
    "    dataset =  remove_duplicates(dataset)\n",
    "\n",
    "    # Check and handle outliers from the database\n",
    "    dataset = handle_outliers(dataset)\n",
    "\n",
    "    # Encoding categorical features using one-hot encoding and label encoding\n",
    "    dataset = encode_categorical_features(dataset)\n",
    "\n",
    "    # Re-Removing the white spaces from feature names\n",
    "    dataset.columns = dataset.columns.str.strip()\n",
    "\n",
    "    # Splitting the dataset into X and y\n",
    "    X, y = split_dataset_to_X_y(dataset)\n",
    "\n",
    "    # Generating synthetic data using SMOTE\n",
    "    X, y = handle_class_imbalance(X, y)\n",
    "\n",
    "    # Splitting the dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Feature scaling using min max scaler\n",
    "    X_train, X_test = scale_features(X_train, X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:46:00.136272Z",
     "start_time": "2025-02-19T19:45:58.794846Z"
    }
   },
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_dataset(dataset)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:46:00.152354Z",
     "start_time": "2025-02-19T19:46:00.148444Z"
    }
   },
   "source": [
    "def preform_grid_search(model, parameters, X_train, y_train):\n",
    "    grid_search = GridSearchCV(model, parameters)\n",
    "    return grid_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:46:00.168031Z",
     "start_time": "2025-02-19T19:46:00.163517Z"
    }
   },
   "source": [
    "def preform_random_search(model, params, n_tier, cv, X_train, y_train):\n",
    "    random_search = RandomizedSearchCV(model, param_distributions=params, n_iter=n_tier, cv=cv, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "    return random_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:46:11.555613Z",
     "start_time": "2025-02-19T19:46:00.180630Z"
    }
   },
   "source": [
    "\n",
    "XGM = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# params_XG = {\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'learning_rate': [0.1, 0.01, 0.001],\n",
    "#     'reg_alpha': [0, 0.1, 0.5, 1, 5, 10],\n",
    "#     'reg_lambda': [0.1, 0.01, 0.001],\n",
    "#     'n_estimators': [100, 200, 350, 400],\n",
    "#}\n",
    "# XGM = preform_grid_search(XGM, params_XG, X_train, y_train)\n",
    "\n",
    "params_XG = {\n",
    "    'max_depth': randint(2, 8),\n",
    "    'reg_alpha': uniform(0.01, 1),\n",
    "    'reg_lambda': uniform(0.01, 10),\n",
    "    'n_estimators': randint(100, 600),\n",
    "}\n",
    "\n",
    "XGM = preform_random_search(XGM, params_XG, 20, 5, X_train, y_train)\n",
    "XGM.best_params_\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2,\n",
       " 'n_estimators': 266,\n",
       " 'reg_alpha': np.float64(0.02326496115986653),\n",
       " 'reg_lambda': np.float64(9.432017556848526)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:46:11.616907Z",
     "start_time": "2025-02-19T19:46:11.599776Z"
    }
   },
   "source": "joblib.dump(XGM.best_estimator_, \"./models/XGBOOST_classifier.pkl\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/XGBOOST_classifier.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:46:11.704090Z",
     "start_time": "2025-02-19T19:46:11.657018Z"
    }
   },
   "source": [
    "y_pred_XG = XGM.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_XG))\n",
    "print(classification_report(y_test, y_pred_XG))\n",
    "print(confusion_matrix(y_test, y_pred_XG))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9096045197740112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91      1633\n",
      "         1.0       0.93      0.88      0.90      1553\n",
      "\n",
      "    accuracy                           0.91      3186\n",
      "   macro avg       0.91      0.91      0.91      3186\n",
      "weighted avg       0.91      0.91      0.91      3186\n",
      "\n",
      "[[1533  100]\n",
      " [ 188 1365]]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:46:28.132040Z",
     "start_time": "2025-02-19T19:46:11.758756Z"
    }
   },
   "source": [
    "LGB = lgb.LGBMClassifier()\n",
    "\n",
    "params_LGB = {\n",
    "    'learning_rate': uniform(0.01, 1),\n",
    "    'max_depth': randint(2, 20),\n",
    "    'num_leaves': randint(20, 60),\n",
    "}\n",
    "\n",
    "LGB = preform_random_search(LGB, params_LGB, 20, 5, X_train, y_train)\n",
    "LGB.best_params_\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6410, number of negative: 6330\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 12740, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503140 -> initscore=0.012559\n",
      "[LightGBM] [Info] Start training from score 0.012559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': np.float64(0.2510254660260117),\n",
       " 'max_depth': 8,\n",
       " 'num_leaves': 27}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:46:28.178609Z",
     "start_time": "2025-02-19T19:46:28.165765Z"
    }
   },
   "source": "joblib.dump(LGB.best_estimator_, \"./models/lightgbm_classifier.pkl\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/lightgbm_classifier.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:46:28.229316Z",
     "start_time": "2025-02-19T19:46:28.205403Z"
    }
   },
   "source": [
    "y_pred_LGM = LGB.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_LGM))\n",
    "print(classification_report(y_test, y_pred_LGM))\n",
    "print(confusion_matrix(y_test, y_pred_LGM))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9074074074074074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.93      0.91      1633\n",
      "         1.0       0.92      0.88      0.90      1553\n",
      "\n",
      "    accuracy                           0.91      3186\n",
      "   macro avg       0.91      0.91      0.91      3186\n",
      "weighted avg       0.91      0.91      0.91      3186\n",
      "\n",
      "[[1521  112]\n",
      " [ 183 1370]]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:48:06.971101Z",
     "start_time": "2025-02-19T19:46:28.275290Z"
    }
   },
   "source": [
    "RF = RandomForestClassifier(\n",
    "    n_estimators = 300\n",
    ")\n",
    "\n",
    "# params_RF = {\n",
    "#     'n_estimators': [100, 300, 500],\n",
    "# }\n",
    "\n",
    "params_RF = {\n",
    "    'max_depth': randint(3, 20),\n",
    "    'min_samples_split': randint(2, 20)\n",
    "}\n",
    "\n",
    "# RF = preform_grid_search(RF, params_RF, X_train, y_train)\n",
    "RF = preform_random_search(RF, params_RF, 20, 10, X_train, y_train)\n",
    "RF.best_params_"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 17, 'min_samples_split': 8}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:48:07.179287Z",
     "start_time": "2025-02-19T19:48:07.052972Z"
    }
   },
   "source": "joblib.dump(RF.best_estimator_, \"./models/randomForest_classifier.pkl\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/randomForest_Classifier.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:48:07.303773Z",
     "start_time": "2025-02-19T19:48:07.188249Z"
    }
   },
   "source": [
    "y_pred_RC = RF.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_RC))\n",
    "print(classification_report(y_test, y_pred_RC))\n",
    "print(confusion_matrix(y_test, y_pred_RC))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9030131826741996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.93      0.91      1633\n",
      "         1.0       0.93      0.87      0.90      1553\n",
      "\n",
      "    accuracy                           0.90      3186\n",
      "   macro avg       0.90      0.90      0.90      3186\n",
      "weighted avg       0.90      0.90      0.90      3186\n",
      "\n",
      "[[1526  107]\n",
      " [ 202 1351]]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-19T19:48:07.327109Z"
    }
   },
   "source": [
    "votes = VotingClassifier(estimators=[('xg', XGM), ('lgb', LGB), ('rf', RF)], voting='soft')\n",
    "votes.fit(X_train, y_train)\n",
    "y_pred= votes.predict(X_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6410, number of negative: 6330\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 12740, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503140 -> initscore=0.012559\n",
      "[LightGBM] [Info] Start training from score 0.012559\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T18:56:24.238424Z",
     "start_time": "2025-02-19T18:56:23.526422Z"
    }
   },
   "cell_type": "code",
   "source": "joblib.dump(votes, \"./models/voting_classifier.pkl\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/voting_classifier.pkl']"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 293
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T18:25:19.734262600Z",
     "start_time": "2025-02-19T18:22:47.640290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9168236032642813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.95      0.92      1633\n",
      "         1.0       0.94      0.89      0.91      1553\n",
      "\n",
      "    accuracy                           0.92      3186\n",
      "   macro avg       0.92      0.92      0.92      3186\n",
      "weighted avg       0.92      0.92      0.92      3186\n",
      "\n",
      "[[1546   87]\n",
      " [ 178 1375]]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
